---
title: "Null Hypothesis Significance Tests"
format:
  revealjs:
    theme: [beige, NHST_presentation.css]
    smaller: true
    slide-number: c/t
    show-slide-number: all
    transition: slide
    incremental: true
    scrollable: true
    footer: "ISMIR 2024: Humans at the Center of MIR --- Null Hypothesis Significance Tests"
    navigation-mode: vertical
    width: 1200
author: "Nat Condit-Schultz"
---

```{r}
library(data.table)
library(stringr)
library(broom)
library(humdrumR)
par(bg = "#fdf6e3")
```

# Data Analysis

## What is the goal of data analysis?

+ Extracting **Truth** from data?
  + **NO!**
+ To support *valid* research claims,
  + **YES!**
  + by...
    + being **rigorously skeptical** of our own data and analyses;
    + considering alternative explanations of observed relationships;
    + recognizing and stating our assumptions;
    + genuinely trying to **falsify** our hypotheses.

# Null Hypothesis Significance Tests

## Motivation

+ NHST is...
  + A crucial opportunity to falsify our hypotheses;
  + **the** standard approach to experimental statistics across many fields of research;
    + A validly-conducted "significant" test is (essentially) required to publish a positive claim about your hypothesis.


## Core Idea

+ The *Null Hypothesis* ($H_0$), is that there are no **true** relationship between independent and dependent variable(s).
  + Even if $H_0$ is true, *apparent relationships may arise purely by random chance*. 
+ If we claim a relationship is real when it is not we make a Type 1 error.   
  + NHST is a mechanism for limiting Type 1 errors.


## Core Method {.smaller}

::: {.panel-tabset}

### A priori


1. Pick an "$\alpha$ level" (acceptable Type 1 error rate).
   + Just kidding; $\alpha := 0.05$
2. Describe a statistical model of the relationship(s) between experimental variables.
   + $\beta_0 + \beta_1X + \epsilon = Y$
3. Characterize the Null Hypothesis in the context of that model.
   + Usually, something like $H_0: \beta_1 = 0$.

### A posteriori

4. Gather data.
5. Compute probability of observed data, or “more extreme” data, *given* $H_0$.
   + This is a $p$ value.
6. Evaluate:
   + If $p \leq \alpha$, your result is considered "*significant*."
   + If $p > \alpha$, your result is "not significant."


:::

## Interpretation

+ If $p \leq \alpha$, 
  + you may claim that your hypothesis has been supported by the data.
+ If $p > \alpha$, 
  + you must acknowledge that the data is not consistent with your hypothesis.
    + Your research claim is (tentatively) falsified.
    + To argue otherwise will be an uphill battle.
    
---

### Rationale

Comprehensive use of NHSTs assures that at most 5% of positive (significant) research claims are Type 1 errors.

+ It does not...
  + *Prove* anything, either way.
  + Demonstrate that a relationship (effect) is strong or important.
  + Tell us about Type 2 errors.


## Practical

What does this all look like in practice?

1. Determine which "statistical test" is appropriate for your data and $H_0$.
2. Find statistics library which includes this test.
   + `R` is simplest place to look.
3. Load data into R/Python/etc.
4. Run test on data.
5. Interpret output, and put into your paper.
   + Is the test significant? (yes/no)
   + Include appropriate details.
   
---

I'll now walk through some examples, using R.

+ We will start with basic ("baby step") examples, before finishing with a review of real *best practices*.



# Statistical Tests {.smaller}


To conduct a NHST, we need to compute $p$ values for our data, given our Null Hypothesis.

+ Basically, we need a function $f(\textbf{IV}_{n\times k},\ \textbf{DV}_{n \times j},\ H_0) = f(Data\ |\ H_0) = p$.
  + Where,
    + $IV_k$ is $k$ independent variables.
    + $DV_j$ is $j$ dependent variables ($j := 1$ for this talk).
    + $n$ is the number of data observations.
+ Every "statistical test" is based on such a function.

---


To compute $p$, all tests compute a **test statistic** with a known probability distribution.

+ For example: 
  + $ANOVA(\textbf{IV}_{n\times k},\ \textbf{DV}_{n}) = F$
  + $F \sim \mathcal{F}(\frac{k - 1}{n - k})$
    + We can compute $p(F)$, using the $\mathcal{F}(\frac{k - 1}{n - k})$ distribution.
+ Test statistics can also be simulated using your data ("bootstrapping").


## (Non)parametric?

::: {.panel-tabset}

### Parametric 

Most statistical tests are based on "parametric" models, modeling relationships between variables and making assumptions about the distribution of error in the data.

+ Variants of the *general linear model* are the most common, by far.
  + $\beta_0 + \beta_1X + \epsilon = Y$
  + $\epsilon \sim \mathcal{N}(0, \sigma)$
+ Test statistics may either represent particular model parameters ($t$) or the improvement of fit when comparing two models ($F$, $\chi^2$).

### Non-parametric

Other tests are "non-parametric," making fewer/no assumptions about the data.

+ They are less *powerful*, but more *robust* ("safer").
+ Examples include...
  + [Pearson's $\chi^2$ test](https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test).
  + The [Mann-Whitney U test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test).


:::


---

## Choosing a Test

::: {.panel-tabset}

### General considerations 

The choice of test depends on the nature of your independent and dependent variables.

+ Generally, the general linear model can accommodate various combinations of variables, so various GLM-based tests are the default approach.


### Independent Variable(s)

Generally, any combination of independent variables can be used.

+ Categorical, numeric, etc.
  + The interpretation of model parameters changes, but the test is the same.
+ Interactions? 
+ Covariates?
  + Include any covariates that make sense, so the model can control for them.


### Dependent Variable

The main question is the nature of your dependent variable.

+ What kind of variable?
  + Continuous, real numbers?
  + Categories? Ordered categories?
  + Rank/Count data?
+ What does error look like?
  + Is normal distribution plausible model?
     

:::

---

In R, we write `lm(DV ~ IV)`.

Additional variables, can be added:

+ `lm(DV ~ IV1 + IV2)`
+ `lm(DV ~ IV1 + IV2 + IV1:IV2)`
+ The default assumption is that DV is a continuous variable, with normal error.
+ The IVs can be any types of variables.
  + If the IVs are categorical, we have a $t$-test (single binary IV) or ANOVA.
  + If the DVs are continuous, we have a linear regression, or multiple regression.
  + We can mix categorical/continuous as well.


# Examples

https://posit.cloud/content/8798770

(Need to make a free account.)

## Quality Evaluation Data

```{r, echo = 1}
SiSEC <- read.csv('../SiSEC08_source_separation/SiSEC08_parsed.csv') |> subset(grepl('Algo', Algorithm))

par(cex.axis = .6, pch = 16, cex = .5)

SiSEC <- as.data.table(SiSEC)
SiSEC[, Order := rank(Ratingscore + rnorm(length(Ratingscore), 0, 40)), by = Listener]
SiSEC <- SiSEC[order(SiSEC$Listener, SiSEC$Order), ]


```


**SiSEC** dataset.

+ Fourteen participants.
+ Nine test conditions (algorithms).
+ Fourteen base audio recordings (8 speech, 6 music).
+ $N = 14 * 9 * 14 = 1764$ total ratings.

---

How do we approach this using NHST?

+ First things first: **what is the Null Hypothesis?**
+ Presumed hypothesis is that some algorithms perform better than others.
+ Therefore, $H_0$: *All nine algorithms actually perform equally well*.
  + This called the "omnibus null hypothesis."
+ What is appropriate test?

----

::: {.panel-tabset}

### Dependent Variable

Ratings range from 0 to 100.

```{r}
par(bg = "#FFFFFF00")
draw(SiSEC$Ratingscore, col = 2, xlab = 'Quality rating')

```

+ Continuous numeric DV, with plausibly normal distribution.


### Independent Variable(s)

+ Main IV is categorical with nine levels (nine algorithms).
+ Possible covariate is the type of stimuli: music vs speech (binary variable).
+ There could plausibly be interactions between algorithm and target/type.
  + The Null Hypothesis would have these interactions as 0.



:::


+ This variant of the linear model is called ANOVA. 
  + $9\times 2$ ANOVA 
  + The ANOVA is based on a $F$ test, comparing the fit of the model with/without each variable.
  + $H_0$ is that the mean value for the DV is the same in all nine IV levels.


---

#### ANOVA


If $H_0$ true, ratio of between-group and with-group variance should follow the $F(\frac{df_1}{df_2})$ distribution.
    
    
```{r, out.width=800}
# library(gifski)
# 
# save_gif(gif_file = 'images/ANOVAs.gif', delay = .5, res = 320, height = 1200, width = 2400, progress = TRUE, {
#   for (i in 1:100) {
#     set.seed(i)
#     ratings <- SiSEC$Ratingscore |> sample()
#     fit <- lm(ratings ~  Algorithm * Type, data = SiSEC)
#     draw(SiSEC$Algorithm, ratings, xlab = '', ylab = 'Rating', pch = 16, bg = '#ffffff00', quantiles = c())
#     mtext(bquote(F[1755]^8 == .(round(summary(fit)$fstatistic[1], 2))), 1, cex = 1.32, line = 4, xpd =T)
#   }
# 
# })

knitr::include_graphics('images/ANOVAs.gif')
```

```{r, out.width = 800}
# save_gif(gif_file = 'images/ANOVAs_F.gif', delay = .5, res = 320, height = 1200, width = 2400, progress = TRUE, {
#   allfs <- c()
#   for (i in 1:100) {
#     set.seed(i)
#     ratings <- SiSEC$Ratingscore |> sample()
#     fit <- lm(ratings ~ Algorithm * Type, data = SiSEC)
# 
#     Fs <- summary(fit)$fstatistic[1]
#     par(bg = '#ffffff00', mar = c(2,4,0,4))
#     curve(df(x, 8, 1755), 0, 10, n = 1000, col = 'blue', xlim = c(0, 10),
#           xlab = quote(F[1755]^8), ylab = quote(paste(Density, '|', H[0])),
#           axes = FALSE)
#     points(Fs, df(Fs, 8, 1755), col = 'red', pch = 4)
#     if (i > 1) points(allfs, df(allfs, 8, 1755), col = 'grey50', cex=.5, pch = 4)
#     allfs <- c(allfs, Fs)
#      axis(1, seq(0, 10, 1), tick = FALSE)
#     axis(2, seq(0, 1, .2), tick = FALSE, las = 1)
#   }
# 
# })
knitr::include_graphics('images/ANOVAs_F.gif')
 

```

---    


```{r}
SiSEC |> with(draw(Algorithm, Ratingscore, bg = '#ffffff00'))
```



----

    
    

```{r eval = FALSE, echo = TRUE, comment=""}
lm(Ratingscore ~ Algorithm * Type, data = SiSEC) |> anova()
```

```{r}
fit <- lm(Ratingscore ~ Algorithm*Type, data = SiSEC)

anova(fit) -> tab 

tab[1:3, 'Pr(>F)'] <- c(format(tab[1, 'Pr(>F)'], scientific = TRUE)  |> str_replace('\\.([0-9][0-9])[0-9]*', '.\\1'),
                     format(tab[2:3, 'Pr(>F)'], scientific = TRUE, digits = 2))
tab[,'F value'] <- round(tab[, 'F value'], 2)
tab[is.na(tab)] <- ''

tab |> knitr::kable(digits = 2, format.args = list(big.mark = ','))
Fs <- anova(fit)['Algorithm','F value']

par(bg = '#ffffff00')
curve(df(x, 8, 1746), 0, Fs, n = 1000, col = 'blue', xlim = c(0, 100),
      xlab = quote(F[1755]^8), ylab = quote(paste(Density, '|', H[0])),
      axes = FALSE)
curve(df(x, 8, 1746), Fs, 100, n = 1000, col = 'red', xlim = c(0, 100), add =TRUE)
axis(1, seq(0, 100, 10), tick = FALSE)
axis(2, seq(0, 1, .2), tick = FALSE, las = 1)

points(Fs[1], df(Fs[1], 8, 1755), col = 'red')



```


+ As we suspected, the omnibus Null Hypothesis does not seem very plausible.

## Emotion Data


```{r, echo = 1}
emote <- fread('../Eerola_emotion/mean_ratings_set2.csv')
```

Eerola's **Music and emotion stimulus sets consisting of film soundtracks**.


+ Eight emotional dimensions
  + `r paste(str_to_title(colnames(emote)[2:9]), collapse = ', ')`.
+ 110 excerpts, validated to match these eight target categories.
+ Participants rated each emotion on scale of 1--9.

---


*Hypothesis*: Perceived energy is related to tension.

How do we approach this using NHST?

+ $H_0$: There is no relationship between energy and tension ratings.
+ What is appropriate test?
    
----


::: {.panel-tabset}

### Dependent Variable

Energy ratings from 0 to 9.

```{r}
par(bg = "#FFFFFF00")
draw(, emote$Energy, col = 2, ylab = 'Energy rating', ylim = c(1, 9))

```

+ Sort-of continuous numeric DV, with (sort of) plausibly normal distribution?


### Independent Variable(s)

Main IV is tension, but all eight emotion ratings are correlated:

```{r, echo = TRUE, eval = FALSE}
cor(emote[ , 2:9])
```

```{r}
cor(emote[, 2:9]) |> format(digits = 2) -> cortab 
colnames(cortab) <- rownames(cortab) <- str_to_title(colnames(cortab))
cortab[upper.tri(cortab)] <- ''

cortab |> knitr::kable()

```

+ These correlations could lead to spurious results.
  + We can control for them by including all seven independent predictors;
  + This isolates the effect of tension, independent of other ratings.
+ There could plausibly be interactions as well...



:::

---


This variant of the linear model is called Multiple Regression. 
  
+ If we don't include interactions, we can test the effect using the $t$ statistic associated with the tension variable.
  + $H_0$ is that the "slope" between tension and energy is zero ($b_{tension} = 0$).
+ However, in more complex cases, it would be easier to again use an $F$ test, comparing the fit of the model with/without each variable.


    
----

#### Linear Regression

If $H_0$ is true, standardized regression coefficient should follow Student's $t(df)$ distribution.


```{r, out.width=1000}
# library(gifski)
# 
# save_gif(gif_file = 'images/ts.gif', delay = .5, res = 320, height = 1200, width = 2400, progress = TRUE, {
#   for (i in 1:100) {
#     set.seed(i)
#     energy_rand <- emote$energy |> sample()
#     fit <- lm(energy_rand ~ . , data =emote[,c(2,4,5,6,7,8,9)])
#     draw(emote$tension, energy_rand, xlab = 'Tension', ylab = 'Energy', alpha = 1, col = 2, bg = '#ffffff00', ylim = c(1,9), lm = TRUE)
# 
#     Ts <- coef(summary(fit))['tension', 't value']
#     mtext(bquote(t[352] == .(round(Ts, 2))), 1, cex = 1.32, line = 6, xpd =T)
#   }
# 
# })

knitr::include_graphics('images/ts.gif')
```

```{r, out.width = 800}
# library(gifski)
# save_gif(gif_file = 'images/ts_t.gif', delay = .5, res = 320, height = 1200, width = 2400, progress = TRUE, {
#   allts <- c()
#   for (i in 1:100) {
#     set.seed(i)
#     energy_rand <- emote$energy |> sample()
#     fit <- lm(energy_rand ~ . , data =emote[,c(2,4,5,6,7,8,9)])
# 
#     Ts <- coef(summary(fit))['tension', 't value']
#     par(bg = '#ffffff00', mar = c(2,4,0,4))
#     curve(dt(x, 352), -5, 5, n = 1000, col = 'blue', xlim = c(-5, 5),
#           xlab = quote(t[352]), ylab = quote(paste(Density, '|', H[0])),
#           axes = FALSE)
#     points(Ts, dt(Ts, 352), col = 'red', pch = 4)
#     if (i > 1) points(allts, dt(allts, 352), col = 'grey50', cex=.5, pch = 4)
#     allts <- c(allts, Ts)
#      axis(1, seq(-5, 5, 1), tick = FALSE)
#     axis(2, seq(0, 1, .2), tick = FALSE, las = 1)
#   }
# 
# })
knitr::include_graphics('images/ts_t.gif')
 

```



---

```{r}

with(emote, draw(Tension, Energy, xlab = 'Tension', ylab = 'Energy', alpha = 1,
                 col = 2, bg = '#ffffff00', lm = T))

fit <- lm(Energy ~ ., data = emote[ , 2:9])
xs <- seq(1, 9, .1)
pred <- predict(fit, newdata = do.call('data.frame', c(list(Tension = xs), colMeans(emote[, c(2,4:9)]))), interval = 'confidence')

points(xs, pred[, 'fit'], type = 'l', lwd = 2, col = 2)
points(xs, pred[, 'lwr'], type = 'l', lwd = 1, col = 2, lty = 'dashed')
points(xs, pred[, 'upr'], type = 'l', lwd = 1, col = 2, lty = 'dashed')

```
  
----


```{r, echo = TRUE, results='hide'}
lm(Energy ~ Tension + Valence + Anger + Fear + Happy + Sad + Tender , data = emote[ , 2:9]) |> summary()
```

```{r, out.width = 800}
fit <- lm(Energy ~ . , data = emote[ , 2:9])

summary(fit) |> tidy(conf.int = TRUE) -> tab

tab$estimate <- round(tab$estimate, 2)
tab$std.error <- round(tab$std.error, 2)
tab$statistic <- round(tab$statistic, 2)
tab$conf.low <- round(tab$conf.low, 2)
tab$conf.high <- round(tab$conf.high, 2)
tab  |> knitr::kable()


Ts <- coef(summary(fit))['Tension', 't value']

par(bg = '#ffffff00')
curve(dt(x, 352), -5, Ts, n = 1000, col = 'blue', xlim = c(-5, 15),
      xlab = quote(t[352]), ylab = quote(paste(Density, '|', H[0])),
      axes = FALSE)
curve(dt(x, 352), Ts, 100, n = 1000, col = 'red', xlim = c(0, 100), add =TRUE)
axis(1, seq(-4, 14, 2), tick = FALSE)
axis(2, seq(0, 1, .2), tick = FALSE, las = 1)

points(Ts, dt(Ts, 352), col = 'red')
```



<!-- #### Logistic Regression -->

<!-- How well did Eerola's participants' judgement of valence match with the target valence? -->

<!-- + We could divide the target emotions into positive and negative categories. -->

<!-- ```{r, echo = TRUE, comment = '', eval = TRUE} -->
<!-- emote_valence <- emote |> filter(!grepl('MODERATE', TARGET))  -->
<!-- emote_valence$POSITIVE <- emote_valence$TARGET %in% c("HAPPY_HIGH", "TENDER_HIGH", "VALENCE POS HIGH", "ENERGY POS HIGH", "TENSION POS HIGH" ) -->


<!-- ``` -->

<!-- ---- -->

<!-- For our new *binary* DV, (linear) multiple regression doesn't make sense. -->

<!-- + Instead, we can use a (multiple) *Logistic Regression* model, which models binomial error. -->
<!--   + In `R`, this can be fit using `glm(DV ~ IV + ..., family = binomial())`. -->


<!-- ```{r echo = TRUE, eval = FALSE, comment = ""} -->
<!-- glm(POSITIVE ~ Anger  + Valence + Tension + Fear + Happy + Sad + Tender,  -->
<!--     family = binomial(), -->
<!--     data = emote_valence) |> anova(test = 'Chisq') -->

<!-- ``` -->

<!-- ```{r} -->
<!-- emote_valence <- emote |> filter(!grepl('MODERATE', TARGET))  -->
<!-- emote_valence$POSITIVE <- emote_valence$TARGET %in% c("HAPPY_HIGH", "TENDER_HIGH", "VALENCE POS HIGH", "ENERGY POS HIGH", "TENSION POS HIGH" ) -->

<!-- glm(POSITIVE ~ Anger  + Valence + Tension + Fear + Happy + Sad + Tender,  -->
<!--     family = binomial(), -->
<!--     data = emote_valence) |> anova(test = 'Chisq') |> knitr::kable() -->

<!-- ``` -->

<!-- + Logistic regression is tested using a $\chi^2$ test statistic. -->


<!-- --- -->


<!-- Logistic regression maps a latent linear regression into a probability. -->

<!-- ```{r} -->
<!-- par(bg = '#ffffff00') -->
<!-- with(emote, plot(Valence, as.integer(POSITIVE), pch = 16, axes = FALSE, xlim=c(1,9), -->
<!--                  col = ifelse(POSITIVE, 'blue', 'black'), xlab = 'Valence rating', ylab = "Valence of Target")) -->
<!-- axis(1, seq(1, 9), tick = FALSE) -->
<!-- axis(2, c(0, 1), c('Negative Target', 'Positive Target'), tick = FALSE, las = 1) -->

<!-- logFit <- glm(POSITIVE ~ Anger  + Valence + Tension + Fear + Happy + Sad + Tender,  -->
<!--     family = binomial(), -->
<!--     data = emote_valence) -->

<!-- xs <- seq(1,9,.01) -->
<!-- newdata <- with(emote, data.frame(Tension = xs, -->
<!--                                   Fear = mean(Fear), -->
<!--                                   Energy = mean(Energy), -->
<!--                                   Valence = mean(Valence), -->
<!--                                   Anger = mean(Anger), -->
<!--                                   Happy = mean(Happy), -->
<!--                                   Sad = mean(Sad), -->
<!--                                   Tender = mean(Tender))) -->

<!-- points(xs, predict(logFit, newdata=newdata, type = 'response'), type = 'l') -->




<!-- ``` -->



# Our Data

For this tutorial, we gathered data from you, rating some of the same excerpts used in the Eerola dataset!

+ Let's see how our data, based on a much smaller sample size, compare to Eerola's.




```{r}
emote_ours <- fread('emotion_data')
emote_ours$Scale <- stringr::str_to_title(emote_ours$Scale)

emote2 <- emote |> filter(Number %in% emote_ours$Excerpt) 
emotions <- colnames(emote)[2:9]
```

---

### By excerpt

```{r}

for (emotion in emotions) {
emote_ours$Excerpt <- as.integer(factor(emote_ours$Excerpt))
emote2$Excerpt <- as.integer(factor(emote2$Number))

emote_ours |> filter(Scale == emotion) |> group_by(Excerpt) |> 
  summarize(MU = mean(Rating)) |>
  with({
    plot(MU ~ Excerpt, axes = FALSE, ylim = c(1,9), type ='b',
         xlab = 'Excerpt', ylab = emotion, pch = 16)
    axis(1, Excerpt, tick = FALSE)
    axis(2, 1:9, tick = FALSE, las = 1)
     legend('topright', pch = c(16, 1), bty='n', col = c('black', 'blue'), legend = c('ISMIR2024', 'Eerola'))
    
    })
  

emote2 |>
  with({ 
    
     points(Excerpt, emote2[[emotion]], ylim = c(1,9), type='b', col = 'blue')
    })
  
}

```



# NHST pitfalls


Let's explore some potential, known challenges with NHST.

----

In an ANOVA, the omnibus test is not really very interesting.
We often want to ask questions about more specific differences between categories.

```{r out.width=800}
SiSEC |> with(draw(Algorithm, Ratingscore, bg = '#ffffff00', pch = 16))
```


For example, what if we want to know if the *best* algorithm is significantly better than the *rest* of the algorithms as a whole?

----




```{r}

SiSEC$BestAlgo <- SiSEC$Algorithm == 'Algo22'

fit <- lm(Ratingscore ~ BestAlgo * Type, data = SiSEC)

lm(fit) |> summary() |>   tidy(conf.int = TRUE) -> tab


tab$estimate <- round(tab$estimate, 2)
tab$std.error <- round(tab$std.error, 2)
tab$statistic <- round(tab$statistic, 2)
tab$conf.low <- round(tab$conf.low, 2)
tab$conf.high <- round(tab$conf.high, 2)
tab  |> knitr::kable()
```

+ This is significant.
+ However, there is a problem...
  + We did not identify this "best algorithm" *a priori*.
  + And the NHST is sensitive to this.
  
---

Imagine that the Null Hypothesis *were* true: all algorithms performed equally well.

+ Every time we sample responses, one algorithm would always perform "best" in the sample, just by chance.
  + By picking this one out, we are (by definition) biasing the result.
  
---

We can simulate this; I sampled from the null hypothesis 1000 times, each time comparing the highest group to the rest and computing a $p$ value.
The result is this:

```{r, cache = TRUE}
Nsim <- 1000

# pvalues <- sapply(1:Nsim, \(i) {
#   SiSEC$RandomRating <- sample(SiSEC$Ratingscore)
#   
#   sortedMeans <- with(SiSEC, tapply(RandomRating, Algorithm, mean)) |> sort(decreasing = TRUE)
#   SiSEC$RandomBest <- SiSEC$Algorithm == names(sortedMeans)[1]
#   
#   fit <- lm(RandomRating ~ RandomBest, data = SiSEC)
#   
#   pvalue <- summary(fit)$coef[2, 4]
#   pvalue
# })
# save(pvalues, file = 'simulation_bestrest.Rd')
load('simulation_bestrest.Rd')

plot(seq(0, 1, length.out = Nsim), sort(pvalues), xlab = 'Quantile (True p)', pch = 16, cex = .5,
     col = ifelse(sort(pvalues) <= .05, 'red', 'black'), axes =FALSE, ylab = 'Predicted p')
axis(1,seq(0,1,.1), tick = FALSE)
axis(2, seq(0,1,.1), las = 1, tick = FALSE)

abline(h = .05, col = 'red', lty='dashed')

Type1errorRate <- mean(pvalues <= .05)
text(Type1errorRate, .1, bquote("" %<-% .(paste0(Type1errorRate * 100, '%'))), col = 'red')



```

+ $p < .05$ in more than 20% of simulations! 


---

### Procedural sensitivity

In general, NHST is sensitive to the exact procedure you use.

+ Any procedural decisions that are made *post-hoc* can affect the validity of results.
+ I.e., *if you use your data to to make decisions about the stats/tests*, your $p$-values won't be accurate.

----

The two most common issues are:

+ Post-hoc **stopping condition**
  + If you decide when to stop collecting data based on the data.
+ Post-hoc **comparisons**
  + When you decide which comparisons to make/test.
+ However, as I've demonstrated above, there are other ways post-hoc decisions can invalidate $p$ values.


## Multiple comparisons



A particularly well known problem is the problem of *multiple comparisons* (or "multiple tests").

+ Anytime you conduct multiple NHSTs, the chance that *at least one* test will be significant is higher than $\alpha$.
  + This called the *family-wise error rate*: i.e., the rate of "at least one" Type 1 error in a group.

----

For ANOVA-like data, it is very common to conduct "pair-wise" $t$-tests between every category, so we would know exactly which algorithms perform better than others.

```{r out.width=800}
SiSEC |> with(draw(Algorithm, Ratingscore, bg = '#ffffff00', pch = 16))
```

+ In this case there would be 36 possible comparisons.


---

If we simulate this:

```{r cache=TRUE, out.width=600, fig.width=8}
library(combinat)
Nsim <- 1000
pairs <- combn(unique(SiSEC$Algorithm), 2, simplify = FALSE)

# Nsignificant <- sapply(1:Nsim, \(i) {
#   SiSEC$RandomRating <- sample(SiSEC$Ratingscore)
# 
#   pairedPvalues <- sapply(pairs,
#                           \(pair) {
#                             fit <- lm(RandomRating ~ Algorithm, data = SiSEC |> subset(Algorithm %in% pair))
#                             pvalue <- summary(fit)$coef[2, 4]
#                             pvalue
#                           })
#   sum(pairedPvalues <= .05)
# 
# })
# save(Nsignificant, file = 'simulation_multipleTests.Rd', compress = TRUE)
load('simulation_multipleTests.Rd')
table(Nsignificant) |> prop.table() |> sort(decreasing = TRUE) |> cumsum() -> props
 
plot(1-props, col = ifelse(names(props) == '0', 'black', 'red'), main = 'Proportion of Simulations with\nat Least N Significant Tests', type = 'b',
     ylab = 'Proporiton of Simulations',
     axes = FALSE,
     x = as.integer(names(props)),
     xlab = 'N Significant Tests', ylim = c(0, 1), pch = 16)
axis(1, 0:20, tick = FALSE)
axis(2, seq(0,1,.1), paste0(seq(0,100,10), '%'), las = 1, tick = FALSE)
```

+ More than forty percent of our (Null Hypothesis) simulations have *at least one* significant tests.
  + This is the "family-wise error rate" 
+ This, if we conducted 36 significance tests and consider *one* significant result to be the basis to reject the Null Hypothesis, we would make a Type 1 error more than 40% of the time!

---

### Correcting for Multiple Comparisons

We can "correct" for multiple tests by adjusting our $p$-values (or $\alpha$ level) accordingly.

+ The naive *Bonferroni correction* is simply to use $\frac{\alpha}{T}$, where $T$ is the number of tests. 
  + This highly conservative (overly cautious), because our tests are not statistically independent from each other.
  + There are many other approaches, such as Tukey's Method.

---

The main lesson is that, whenever you conduct multiple NHST tests, you need to take them with an extra skeptical grain of salt.

+ If you conduct many tests (like hundreds or thousands), some of those will have $p < .05$. 


# Data Independence

All statistical models assume that each individual data point is *statistically independent* of every other data point.

+ This assumption is often violated in experimental research.

## Repeated measures

The most common source of data dependence is *repeated measures*:

+ Repeated measurements from the same human participants.
+ Repeated responses to the same stimuli ("items").
  + Repeats of instrument, musical passage, speaker, dialog, etc.

---

::: {.panel-tabset}

### By participant

```{r, echo = FALSE, out.width=1000}
par(las=2, cex.axis=.6)
with(SiSEC, draw(bg = '#ffffff00',quantiles = c(), factor(Listener), Ratingscore, pch = 16),
     xlab = 'Participant', ylab = "Rating")

```

+ Participant 62 tends to give much higher ratings than participant 45.

### By item


```{r, echo = FALSE,out.width=1000}

par(las=2, cex.axis=.6)
with(SiSEC, draw(bg = '#ffffff00', quantiles = c(), factor(paste0(Source, ':', Target)), Ratingscore, pch = 16),
     xlab = 'Target', ylab = "Rating")


```

+ Again, the specific measurement items vary quite a lot in average rating, *across all algorithms*.

:::
  
  
---

In a well-designed experiment like this, every combination of all levels is measured.

+ I.e., every participant heard every item processed by every algorithm.
+ This "full factorial" design minimized the impact of data dependence on our results.
  + However, fully-factorial designs are not always feasible.
  
---

Even in a fully-factorial design, data dependence can be problematic.

+ If I inspect five data points from a given participant, I can use that data to guess what their other data points will be.
  + Thus, each individual data point communicates less information.
  + Thus, the amount of information in the data is actually less than the apparent sample size (`r format(nrow(SiSEC), big.mark=',')`) would indicate.
    + This can make statistical models underestimate the true uncertainty of estimated statistics.
    
## Random-Effects Models

A generalized solution the problem of data dependence is through **mixed effects models**.
Also called "multilevel models" or "random-effects models."

+ In these models, multiple sources of random error are explicitly modeled:
  + A random sample of participants.
  + A random sample of measurement items.
  + Random residual error.
  
---

Mixed-effects models include:

+ *Fixed effects*:
  + These are the "normal" effects we've been using already.
+ *Random effects*:
  + Variation in baselines between participants/items using *random intercepts*.
  + Variation in the effects of independent variables using *random slopes*.
  
---

Random effects represent variables that were sampled randomly.

+ For example, your participants are a random sample of listeners.
+ The musical excerpts you use, are a random sample of (potential) excerpts.
  
----
  
We can use the `lme4` package to fit random-effects models for various versions of the GLM.

+ The syntax builds on the basic R syntax, by simple adding random effects terms to the equation:
  + Random intercepts: `DV + IV + (1 | Group)`
  + Random slopes: `DV + IV + (IV | Group)`
  
---

#### Maximal model

+ The best-practice is to include random slopes for all independent variables in all repeated measure groups:
  + `RatingScore ~ Algorithm + Type + (Algorithm + Type | Listener) + (Algorithm + Type | Target)`
+ However, it may be impossible to fit these "maximal" models if your data is not large enough, so we resort to only random intercepts:
  +  `RatingScore ~ Algorithm + Type + (1 | Listener) + (1 | Target)`

---

### Best-practice

This brings us to the best-practice version of our omnibus ANOVA test:

```{r echo = TRUE, eval = FALSE, comment = ""}
library(lme4)
library(lmerTest)

mixedFit <- lmer(Ratingscore ~ Algorithm + Type + (Algorithm | Listener) + (Algorithm | Target),
                 data = SiSEC)

anova(mixedFit)
```

```{r, eval = TRUE, cache = TRUE, echo = FALSE}

# mlvfitnull <- lmer(Ratingscore ~ 1        + Type + (Algorithm | Listener) + (Algorithm | Target),
                   # data = SiSEC, REML = FALSE)
# mlvfit    <- lmer(Ratingscore ~ Algorithm + Type + (Algorithm | Listener) + (Algorithm | Target),
                  # data = SiSEC, REML = FALSE)
# load('fit_quality_mlv.Rd')
load('fit_quality_mlv.Rd')

anova(mlvfit, test = 'F') |> knitr::kable()

```

+ The difference between algorithm ratings is still statistically significant.
  + However, the $F$ statistic for this new model ($F \approx 10$) is much smaller than for the naive ANOVA we conducted earlier ($F \approx 90$), because this model more accurately reflects the true uncertainty in the data.
+ We've been good skeptics!

<!-- --- -->

<!-- For our emotion data, we don't have participant information. -->
<!-- However, we do have information about the (random) experimental stimuli. -->

<!-- xxx -->


## Non-random dependence



Another possibility is for data-dependence to be non-random.


+ A good example is *order effects*.
  + For example, we might look at how participants' ratings evolved over the course of their experimental session and see something like...

---

```{r}

with(SiSEC, draw(Order, Ratingscore, lm = TRUE, col = 2, bg = '#FFFFFF00'))

```



+ Participants' ratings tended to get higher over time!^[The SiSEC data didn't include order information, so I faked this data.]
  + The data is autocorrelated, with $r =$ `r with(SiSEC, cor(head(Ratingscore, -1), tail(Ratingscore, -1))) |> round(2)` at lag 1.
+ As before, it means each data point contains less unique information, because each can be predicted from past ratings.

----

We an explicitly incorporate this autocorrelation into our model:


```{r echo = TRUE, eval = FALSE, comment = ""}
lm(Ratingscore ~ Algorithm * Type + Order, data = SiSEC) |> anova()
```

```{r}

# SiSEC <- within(SiSEC, PreviousRating <- unlist(tapply(Ratingscore, Listener, \(rs) c(NA, head(rs, -1)) )))

lm(Ratingscore ~ Algorithm * Type +  Order, data = SiSEC) -> fitorder

anova(fitorder) -> tab 

tab[1:4, 'Pr(>F)'] <- format(tab[1:4, 'Pr(>F)'], scientific = TRUE)  |> str_replace('\\.([0-9][0-9])[0-9]*', '.\\1')
tab[,'F value'] <- round(tab[, 'F value'], 2)
tab[is.na(tab)] <- ''

tab |> knitr::kable(digits = 2, format.args = list(big.mark = ','))
```

+ Accounting for the order effect actually increased the $F$ value for algorithm!

# Effect Size

---

A statistically significant effect tells us nothing about how strong an effect is.

+ With enough data (or little variability) even small effects can be significant.

---

Effect size should be reported independently of statistical tests.

+ *Use graphs to visualize effect size in meaningful way*.
+ Report standardized metrics, appropriate for your model/test:
  + $r$, $R^2$
  + $\beta$, coefficient of variation, Coen's $d$
+ Report non-standardized metrics:
  + $b$ (regression slopes)

---

Do not use $p$ values as effect-size metrics.

+ $p < .01$, $p < .001$, $p < .0000001$, etc.
+ This is a common practice, but $p$ values are **bad** inferential statistics, and bad measures of effect size.


# Summary

+ NHST is important first step in evaluating experimental results
+ Select a model appropriate for your variables
+ Look out for dependent data
  + Incorporate random effects to account for repeated measures.
  + Explicitly model other sources of dependence.
+ *Do not* make test decisions based on data (*post-hoc*).
  + Stopping condition
  + Multiple comparisons
    + Correct for multiple tests
+ Don't interpret significance tests as measure of effect size.
