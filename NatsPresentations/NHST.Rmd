---
title: "Null Hypothesis Significance Tests"
format:
  revealjs:
    theme: dark
    slide-number: c/t
    show-slide-number: all
    transition: slide
    incremental: true
    scrollable: true
    footer: "ISMIR 2024: Humans at the Center of MIR --- Null Hypothesis Significance Tests"
    navigation-mode: grid
author: "Nat Condit-Schultz"
---

# Data Analysis

## What is the goal of data analysis?

+ To support *valid* research claims.
+ To be **rigorously skeptical** about our data and our models of the data.
  + Recognize assumptions made about data/models.
  + Condiser alternative explanations of observed relationships.
  + Try to disprove (falsify) your models.
+ This is what reviewers expect.

# Null Hypothesis Significance Tests

## Motivation

+ The standard approach to experimental statistics across many fields of research.
+ A crucial opportunity to falsify your hypothesis.
+ A validly-conducted "significant" test is (essentially) required to make a positive claim about your hypothesis.


## Core Idea

+ The *Null Hypothesis* ($H_0$), is that there are no **true** relationship between independent and dependent variable(s).
+ Any *apparent relationships*^[in visualizations, descriptive statistics, or predictive power] may arise purely by random chance. 
+ If we claim a relationship is real when it is not we make a Type 1 error.   
  + NHST is a mechanism for limiting Type 1 errors.


## Core Method {.smaller}

::: {.panel-tabset}

### A priori


1. Pick an "$\alpha$ level" (acceptable Type 1 error rate).
  + Just kidding; $\alpha := 0.05$
2. Describe a statistical model of the relationship(s) between experimental variables.
  + Usually a variant of the General Linear Model.
  + $\beta_0 + \beta_1X + \epsilon = Y$
    + Includes assumptions about distribution of residual error! ($\epsilon$)
3. Characterize the Null Hypothesis in the context of that model.
  + Usually, something like $H_0: \beta_1 = 0$.

### A posteriori

4. Gather data.
5. Compute probability of observed data, or “more extreme” data, given $H_0$.
   + This is a $p$ value.
6. Evaluate:
   + If $p \leq \alpha$, then result is considered significant.
     + Your research claim *may* be valid.
   + If $p > \alpha$, then result is not significant.
     + Your research claim is falsified.
       + (To argue otherwise will be an uphill battle.)


:::

## Interpretation

Following this method assures that at most 5% of positive (significant) research claims are Type 1 errors.

+ It does not...
  + *Prove* anything, either way.
  + Demonstrate that a relationship (effect) is strong, or important.
  + Tell us about Type 2 errors.



# Statistical Models

What does this look like in practice?

+ Find a statistical model that can represent your independent and dependent variables.
  + `DV ~ IV`
+ In fact, when you design your experiment in the first place, consider what statistical model to use.


## {.smaller}

There are many standard models/tests:

:::: {.columns}

::: {.column width='60%'}


+ For many, $p$-values can be computed trivially.

:::

::: {.column width='40%'}

+ Linear regression
+ $t$-test
+ ANOVA
+ MANOVA

::: 

::::

:::: {.columns}

::: {.column width='60%'}

+ Others require algorithmic “fitting” process (equivalent to "training" in ML).

:::

::: {.column width='40%'}

+ Logistic regression 
+ Multinomial regression
+ Multi-level models

::: 
::::

+ All are variants of General Linear Model.

## {.smaller}

The appropriate statistical model/test depends on the type of DV and IVs you have. 
		
DV ~ $IV_1 + (+ IV₂ + … + IVₙ)$


::: {.panel-tabset}


### Numeric DV


|  $IV_{k}$                             | Model                      |                           | Test Statistic |
|:--------------------------------------|:---------------------------|:--------------------------|:---------------|
|  $\mathbb{R}^k$                       | Linear regression          | $k=1 :$ "Simple"          | $t$            |
|                                       |                            | $k>1 :$ "Multiple"        | $t^k$          |
|  $\mathbb{Cat}\{2\}$                  | $t$-test                   |                           | $t$            |
|  $\mathbb{Cat}\{\geq 2\}^k$           | ANOVA                      | $k=1 :$ "One-way"         | $F$            |   
|                                       |                            | $k>1 :$ "{k}-way"         | $F^k$          |
|  $\mathbb{R}^* \times \mathbb{Cat}^*$ | ANCOVA                     |                           | $F^k$          |

+ These models all assume a normally-distributed DV.
+ Tests are based on $t$ or $F$ sampling distributions.


### Categorical DV

|  $DV_{k}$                             | Model                      |                                         | Test Statistic |
|:--------------------------------------|:---------------------------|:----------------------------------------|:---------------|
|  $\mathbb{Cat}\{2\}$                  | Logistic regression        | $\mathbb{Cat}(IV) :$ "Logistic ANOVA"   | $\chi^2$       |
|  $\mathbb{Cat}\{>2\}$                 | Multinomial regression     |                                         | $\chi^2$       |

+ In these models, a latent linear variable is mapped to discrete categories.
+ Evaluated using log-likelihood ratio test.

### Ordinal DV

|  $DV_{k}$                             | Model                      | Test Statistic |
|:--------------------------------------|:---------------------------|:---------------|
|  $\mathbb{Ord}\{>2\}$                 | Ordinal logit regression   | $\chi^2$       |
|  $\mathbb{Cat}\{>2\}$                 | Oridnal probit regression  | $\chi^2$       |

+ In these models, a latent linear variable is mapped to ordinal categories.
+ Evaluated using log-likelihood ratio test.

:::


# Examples


## Fake Data



```{r, echo = TRUE}
library(broom)

N <- 100
X1_R <- rnorm(N)
X2_R <- rnorm(N)
X3_Cat2 <- gl(2, N / 2, labels = c('On', 'Off')) |> sample()
X4_Cat5 <- gl(5, N / 5, labels = LETTERS[1:5])

sigma <- 10

Y <- 5.2 + 1 * X1_R + 
  -11.2 * X1_R + 
  as.integer(X3_Cat2) * 7.2 + 
  (contrasts(X4_Cat5) %*% c(c(1, 3, -10, 5)))[X4_Cat5] +
  rnorm(N, 0, sigma)



```

## {.smaller}


::: {.panel-tabset}

### Linear

```{r}
fit <- lm(Y ~ X1_R)

plot(Y ~ X1_R)
abline(fit)

fit |> tidy()
```

### t-test

```{r}
fit <- lm(Y ~ X3_Cat2)

plot(Y ~ X3_Cat2)
abline(fit)

fit |> tidy()

```

### One-way ANOVA


```{r}
fit <- lm(Y ~ X4_Cat5)

plot(Y ~ X4_Cat5)

fit |> anova() |> tidy()

```

### 2 X 5 ANOVA

```{r}
fit <- lm(Y ~ X3_Cat2 * X4_Cat5)

tapply(Y, list(X3_Cat2, X4_Cat5), mean) |> barplot(beside = TRUE)

fit |> anova() |> tidy()

```


:::


## Quality Evaluation Data

```{r, echo = c(2, 6)}
par(cex.axis = .6, pch = 16, cex = .5)
SiSEC <- read.csv('../SiSEC08/SiSEC08_anonymized.csv') |> subset(grepl('Algo', Condition))
SiSEC <- as.data.table(SiSEC)
SiSEC[, Order := rank(Ratingscore + rnorm(length(Ratingscore), 0, 40)), by = Listener]
SiSEC <- SiSEC[order(SiSEC$Listener, SiSEC$Order), ]
plot(Ratingscore ~ factor(Condition), data = SiSEC)

```

+ The "omnibus" Null Hypothesis would be that nine algorithms perform equally well.
  + This doesn't appear to be the cases, but our skeptical minds must ask whether this spread could happen by chance.
  
---

This is a "basic" one-way ANOVA.

```{r}
fit <- lm(Ratingscore ~ Condition, data = SiSEC)

anova(fit)

```

+ As we suspected, the omnibus Null Hypothesis does not seem very plausible.

----


However, in typical MIR projects, asking whether all the algorithms perform the same wouldn't really be the thing that interests us.

We'd be more likely to want to know whether a particular algorithm performs better than the other algorithms, or better than a the previous "best" algorithm.

---

If we wanted to compare the latest algorithm (34) to the previous iteration (32), we can reduce this to a $t$-test.

+ The Null Hypothesis is that the performance of Algo32 and Algo34 is the same.

```{r}
fit <- lm(Ratingscore ~ Condition, data = SiSEC |> subset(Condition %in% c('Algo32', 'Algo34')))

lm(fit) |> summary()

```

+ Two things to notice:
  + Algorithm 34 actually has slightly *lower* performance on average (`-.2143`).
  + However, the test is not significant, so the Null Hypothesis cannot be rejected.
    + We would not conclude that Algorithm 34 is worse than Algorithm 32.
    
    
---

Perhaps we'd like to know whether the best performing algorithm (22) is really better than the second best (30)?

```{r}

plot(Ratingscore ~ factor(Condition), data = SiSEC)

fit <- lm(Ratingscore ~ Condition, data = SiSEC |> subset(Condition %in% c('Algo22', 'Algo30')))

lm(fit) |> summary()

```

----

Perhaps we'd like to know whether the best performing algorithm (22) is really better than the rest of the algorithms?

```{r}

SiSEC$BestAlgo <- SiSEC$Condition == 'Algo22'

fit <- lm(Ratingscore ~ BestAlgo, data = SiSEC)

lm(fit) |> summary()

```

+ This is significant.
+ However, there is a problem...
  + We did not identify this "best algorithm" *a priori*.
  + And the NHST is sensitive to this.
  
---

Imagine that the Null Hypothesis *were* true: all algorithms performed equally well.

+ Every time we sample responses, one algorithm will perform best in the sample.
  + By picking this one out, we are (by definition) biasing the result.
  
---

We can simulate this. 

```{r}
Nsim <- 1000

pvalues <- sapply(1:Nsim, \(i) {
  SiSEC$RandomRating <- sample(SiSEC$Ratingscore)
  
  sortedMeans <- with(SiSEC, tapply(RandomRating, Condition, mean)) |> sort(decreasing = TRUE)
  SiSEC$RandomBest <- SiSEC$Condition == names(sortedMeans)[1]
  
  fit <- lm(RandomRating ~ RandomBest, data = SiSEC)
  
  pvalue <- summary(fit)$coef[2, 4]
  pvalue
})

plot(seq(0, 1, length.out = Nsim), sort(pvalues), xlab = 'Quantile', 
     col = ifelse(sort(pvalues) <= .05, 'red', 'black'))

abline(h = .05, col = 'red', lty='dashed')

Type1errorRate <- mean(pvalues <= .05)
text(Type1errorRate, .1, bquote("" %<-% .(paste0(Type1errorRate * 100, '%'))), col = 'red')



```

# NHST pitfalls



In general, NHST is sensitive to the exact procedure you use.


:::: {.columns}

::: {.column width='50%'}

+ Any procedural decisions made *post-hoc* and affect the validity of results.
+ I.e., if you use the data to to make decisions about stats/tests.

:::

::: {.column width='50%'}

+ Stopping condition
  + When you stop collecting data.
+ Which comparisons you make.

:::

::::

## Multiple tests

A particularly well known problem is the problem of *multiple tests*.

+ Anytime you conduct multiple NHSTs, the chance that *at least one* test will be significant is higher than $\alpha$.

----

A common technique is to conduct "pair-wise" $t$-tests between every category, so we would know exactly which algorithms perform better than others.

```{r, echo = FALSE}
plot(Ratingscore ~ factor(Condition), data = SiSEC)

```

+ In this case there would be 36 possible comparisons.


---

If we simulate this

```{r}
library(combinat)
Nsim <- 1000
pairs <- combn(unique(SiSEC$Condition), 2, simplify = FALSE)

Nsignificant <- sapply(1:Nsim, \(i) {
  SiSEC$RandomRating <- sample(SiSEC$Ratingscore)
  
  pairedPvalues <- sapply(pairs, 
                          \(pair) {
                            fit <- lm(RandomRating ~ Condition, data = SiSEC |> subset(Condition %in% pair))
                            pvalue <- summary(fit)$coef[2, 4]
                            pvalue
                          })
  sum(pairedPvalues <= .05)
  
  
  

})

table(Nsignificant) |> prop.table() |> sort(decreasing = TRUE) |> cumsum() -> props
 
plot(1-props, col = ifelse(names(props) == '0', 'black', 'red'), main = 'Proportion of Simulations with\nat Least N Significant Tests', type = 'b',
     ylab = 'Proporiton of Simulations',
     axes = FALSE,
     x = as.integer(names(props)),
     xlab = 'N Significant Tests', ylim = c(0, 1), pch = 16)
axis(1, 0:20, tick = FALSE)
axis(2, seq(0,1,.1), paste0(seq(0,100,10), '%'), las = 1, tick = FALSE)
```

+ More than forty percent of our (Null Hypothesis) simulations have *at least one* significant tests.
  + This is the "family-wise error rate" 
+ This, if we conducted 36 significance tests and consider *one* significant result to be the basis to reject the Null Hypothesis, we would make a Type 1 error more than 40% of the time!

## Correcting for Multiple Comparisons

We can "correct" for multiple tests by adjusting our $p$-values (or $\alpha$ level) accordingly.

+ The naive *Bonferroni correction* is simply to use $\frac{\alpha}{T}$, where $T$ is the number of tests. 
  + This highly conservative (overly cautious), because our tests are not statistically independent from each other.
  + There are many other approachess, such as Tukey's Method.

---

The main lesson is that, whenever you conduct multiple NHST tests, you need to take them with an extra skeptical grain of salt.

+ If you conduct many tests (like hundreds or thousands), some of those will have $p < .05$. 

# Data Independence

All statistical models assume that each individual data point is *statistically independent* of every other data point.

+ This assumption is often violated in experimental research.

## Repeated measures

The most common source of data dependence is *repeated measures*:

+ Repeated measurements from the same human participants.
+ Repeated responses to the same stimuli ("items").

---

Take a look at your quality ratings again:

::: {.tabset-panel}

### By participant

```{r, echo = -1:-2}
par(las=2, cex.axis=.6)
with(SiSEC, tapply(Ratingscore, Listener, mean)) |> sort() |> names() -> ranked

plot(Ratingscore ~ factor(Listener, levels = ranked), data = SiSEC,
     xlab = '')

```


+ Participant 62 tends to give much higher ratings than participant 45.

### By item

```{r, echo = -1:-2}
par(las = 2, mar = c(7,5,3,3))
with(SiSEC, tapply(Ratingscore, Trial, mean)) |> sort() |> names() -> ranked

plot(Ratingscore ~ factor(Trial, levels = ranked), data = SiSEC,
     xlab = '')

```
+ Again, the specific measurement items vary quite a lot in average rating, *across all algorithms*.

:::
  
  
---

In a well-designed experiment like this, every combination of all levels is measured.

+ I.e., every participant heard every item processed by every algortithm.
+ This "full factorial" design minimized the impact of data dependence on our results.
  + However, fully-factorial designs are not always feasible.
  
---

Even in a fully-factorial design, data dependence can be problematic.

+ If I inspect five data points from a given participant, I can use that data to guess what their other data points will be.
  + Thus, each individual data point communicates less information.
  + Thus, the amount of information in the data is actually less than the apparent sample size.
    + This can make statistical models underestimate the true uncertainty of estimated statistics.
    
## Random-Effects Models

A generalized solution the problem of data dependence is through random effects models.

+ In these models, multiple sources of random error are explicitely modeled:
  + A random sample of participants.
  + A random sample of measurement items.
  + Within participants/items, random residual error.
+ We can represent:
  + variation in baselines between participants/items using *random intercepts*.
  + variation in the effects of independent variables using *random slopes*.
  
  
+ We can use the `lme4` package to fit random-effects models for various versions of the GLM.


---

We can thus conduct a best-practice version of our omnibus ANOVA test.

```{r, eval = FALSE}
library(lme4)
fitnull <- lmer(Ratingscore ~ 1 + (Condition|Listener) + (Condition|Trial), data = SiSEC, REML = FALSE)
fit <- lmer(Ratingscore ~ Condition + (Condition|Listener) + (Condition|Trial), data = SiSEC, REML = FALSE)


```

+ The difference between algorithm ratings is still statistically significant.
  + The $F$ statistic for this new model ($F \approx 10$) is much smaller than for the naive ANOVA we conducted earlier ($F \approx 80$), because this model more accurately reflects the true uncertainty in the data.
+ We've been good skeptics!

## Non-random dependence



Another possibility is for data-dependence to be non-random.

For example, we might look at how participants' ratings evolved over the course of their experimental session and see something like:

```{r}

with(SiSEC, plot(Ratingscore ~ Order))
abline(lm(Ratingscore ~ Order, data = SiSEC))

```
+ Participants' ratings tended to get higher over time!^[The SiSEC data didn't include order information, so I faked this data.]
  + The data is autocorrelated, with $r =$`r with(SiSEC, cor(head(Ratingscore, -1), tail(Ratingscore, -1))) |> round(2)` at lag 1.
+ As before, it means each data point contains less unique information, because they can be predicted from past ratings.

----

We an explicitly incorporate this autocorrelation into our model:

```{r}

SiSEC <- within(SiSEC, PreviousRating <- unlist(tapply(Ratingscore, Listener, \(rs) c(NA, head(rs, -1)) )))

anova(lm(Ratingscore ~ Condition + PreviousRating, data = SiSEC)) |> tidy()

```

# Effect Size

---

A statistically significant effect tells us nothing about how strong an effect is.

+ With enough data (or little variability) even small effects can be significant.

---

Effect size should be reported independently of statistical tests.

+ Use graphs to visualize effect size in meaningful way.
+ Report standardized metrics:
  + $r$, $R^2$, $\beta$, coefficient of variation, Coen's $d$
+ Report non-standardized metrics:
  + $b$

---

Do not use $p$ values as effect-size metrics.

+ $p < .01$, $p < .001$, $p < .0000001$, etc.
+ This is a common practice, but $p$ values are **bad** inferential statistics, and bad measures of effect size.


# Summary

+ NHST is important first step in evaluating experimental results
+ Find model appropriate for your variables
+ Look out for dependent data
  + Incorporate random effects to account fo repeated measures.
  + Explicitely model other sources of dependence.
+ Post-hoc
  + Do not make decisions based on data.
    + Stopping condition
    + Post-hoc comparisons
  + Correct for multiple tests